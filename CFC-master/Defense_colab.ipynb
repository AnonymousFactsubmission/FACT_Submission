{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Of42Ktq5Qr6",
        "outputId": "2b480ca0-a175-45b6-904a-42ad17532bee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kmedoids\n",
            "  Downloading kmedoids-0.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (468 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/468.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/468.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kmedoids\n",
            "Successfully installed kmedoids-0.5.0\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Collecting python-mnist\n",
            "  Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: python-mnist\n",
            "Successfully installed python-mnist-0.7\n",
            "Collecting pulp\n",
            "  Downloading PuLP-2.8.0-py3-none-any.whl (17.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-2.8.0\n",
            "Collecting scikit-learn==0.22.2.post1\n",
            "  Downloading scikit-learn-0.22.2.post1.tar.gz (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.22.2.post1) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.22.2.post1) (1.11.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.22.2.post1) (1.3.2)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for scikit-learn\n",
            "Failed to build scikit-learn\n",
            "\u001b[31mERROR: Could not build wheels for scikit-learn, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting zoopt\n",
            "  Downloading zoopt-0.4.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from zoopt) (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from zoopt) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->zoopt) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->zoopt) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->zoopt) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->zoopt) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->zoopt) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->zoopt) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->zoopt) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->zoopt) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->zoopt) (1.16.0)\n",
            "Installing collected packages: zoopt\n",
            "Successfully installed zoopt-0.4.2\n",
            "Collecting pyckmeans\n",
            "  Downloading pyckmeans-0.9.4.tar.gz (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyckmeans) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pyckmeans) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyckmeans) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyckmeans) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pyckmeans) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pyckmeans) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyckmeans) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyckmeans) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyckmeans) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyckmeans) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyckmeans) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyckmeans) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyckmeans) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyckmeans) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pyckmeans) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyckmeans) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyckmeans) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->pyckmeans) (1.16.0)\n",
            "Building wheels for collected packages: pyckmeans\n",
            "  Building wheel for pyckmeans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyckmeans: filename=pyckmeans-0.9.4-cp310-cp310-linux_x86_64.whl size=93215 sha256=64b750a1158aebea35e86428298abb5f4323cb65231e8bc744d1dcbfc72b6101\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/83/bb/d0fc31d29f361aff1f400ca2fb2d32c98f900e1aa480229f99\n",
            "Successfully built pyckmeans\n",
            "Installing collected packages: pyckmeans\n",
            "Successfully installed pyckmeans-0.9.4\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install kmedoids\n",
        "!pip install gdown\n",
        "!pip install python-mnist\n",
        "!pip install pulp\n",
        "# !pip install scikit-learn==0.22.2 --upgrade\n",
        "!pip install scikit-learn==0.22.2.post1\n",
        "!pip install zoopt\n",
        "!pip install pyckmeans\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g71HCBjkaJ88",
        "outputId": "2b45dd95-18cf-45e9-bd99-474fdc8e078e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pVOCIQVy7CAF"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# # !fusermount -u drive\n",
        "# # !google-drive-ocamlfuse drive\n",
        "# os.chdir(\"gdrive/My Drive/CFC-Master/Fair-Clustering-Codebase\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwHtZKwz5q20",
        "outputId": "8f50bd7b-72d6-4b4b-c1c9-767e174f1ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/CFC-master/Fair-Clustering-Codebase\n"
          ]
        }
      ],
      "source": [
        "# %cd gdrive/My Drive/CFC-Master/Fair-Clustering-Codebase/\n",
        "%cd /content/gdrive/MyDrive/CFC-master/Fair-Clustering-Codebase\n",
        "#%cd Fair-Clustering-Codebase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mts-RZ9kTT6w",
        "outputId": "fe086641-5eeb-4544-e69d-e69c4b852497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consensus-Fair-Clustering  s_TOY.npy\t\t U_idx_Yale.npy        V_idx_TOY.npy   y_TOY.npy\n",
            "fair_clustering\t\t   U_idx_DIGITS.npy\t utils.py\t       V_idx_Yale.npy\n",
            "models.py\t\t   U_idx_MNIST_USPS.npy  V_idx_DIGITS.npy      X_DIGITS.npy\n",
            "__pycache__\t\t   U_idx_Office-31.npy\t V_idx_MNIST_USPS.npy  X_TOY.npy\n",
            "s_DIGITS.npy\t\t   U_idx_TOY.npy\t V_idx_Office-31.npy   y_DIGITS.npy\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qL6aOpmUSkq9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8' #:4096:8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SfGHMVvV51A9"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import pandas as pd\n",
        "import random\n",
        "import kmedoids\n",
        "from sklearn.decomposition import PCA\n",
        "from zoopt import Dimension, ValueType, Objective, Parameter, Opt, ExpOpt\n",
        "import seaborn as sns\n",
        "import subprocess\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from fair_clustering.eval.functions import * #[TO-DO] Write base class and derive metrics from it, temporary eval code\n",
        "\n",
        "from fair_clustering.dataset import ExtendedYaleB, Office31, MNISTUSPS\n",
        "from fair_clustering.algorithm import FairSpectral, FairKCenter, FairletDecomposition, ScalableFairletDecomposition\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMbLij2c7-Wl",
        "outputId": "bbd671d9-b20d-4ce5-f8df-905144c13da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3800, 256) (3800,) (3800,)\n"
          ]
        }
      ],
      "source": [
        "# Set parameters related to dataset and get dataset\n",
        "\n",
        "name = 'CIFAR100' #Choose between Office-31, MNIST_USPS, Yale, or DIGITS\n",
        "\n",
        "if name == 'Office-31':\n",
        "  dataset = Office31(exclude_domain='amazon', use_feature=True)\n",
        "  X, y, s = dataset.data\n",
        "elif name == 'CIFAR100':\n",
        "  dataset = Cifar100(exclude_domain='omnivore', use_feature=False)\n",
        "  X, y, s = dataset.data\n",
        "elif name == 'MNIST_USPS':\n",
        "  dataset = MNISTUSPS(download=True)\n",
        "  X, y, s = dataset.data\n",
        "elif name == 'Yale':\n",
        "  dataset = ExtendedYaleB(download=True, resize=True)\n",
        "  X, y, s = dataset.data\n",
        "elif name == 'DIGITS':\n",
        "  X, y, s = np.load('X_' + name + '.npy'), np.load('y_' + name + '.npy'), np.load('s_' + name + '.npy')\n",
        "\n",
        "print(X.shape, y.shape, s.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_aXpU_Zq7sFm"
      },
      "outputs": [],
      "source": [
        "# Fairness Defense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "di4DWx-1Vfap"
      },
      "outputs": [],
      "source": [
        "from pyckmeans import CKmeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVaDiOlF1Zq9",
        "outputId": "05830789-1da5-4ec9-e006-7d7f98eba0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consensus-Fair-Clustering  s_TOY.npy\t\t U_idx_Yale.npy        V_idx_TOY.npy   y_TOY.npy\n",
            "fair_clustering\t\t   U_idx_DIGITS.npy\t utils.py\t       V_idx_Yale.npy\n",
            "models.py\t\t   U_idx_MNIST_USPS.npy  V_idx_DIGITS.npy      X_DIGITS.npy\n",
            "__pycache__\t\t   U_idx_Office-31.npy\t V_idx_MNIST_USPS.npy  X_TOY.npy\n",
            "s_DIGITS.npy\t\t   U_idx_TOY.npy\t V_idx_Office-31.npy   y_DIGITS.npy\n"
          ]
        }
      ],
      "source": [
        "# Remember we need the model class in the same directory to load our models so copy those over\n",
        "!cp Consensus-Fair-Clustering/models.py ./\n",
        "!cp Consensus-Fair-Clustering/utils.py ./\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "W3keMG_CDacQ"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import random\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "from models import GMLP, ClusteringLayer\n",
        "from utils import get_A_r, sparse_mx_to_torch_sparse_tensor, target_distribution, aff\n",
        "\n",
        "from scipy import sparse\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vgo6_Z4OLmeT"
      },
      "outputs": [],
      "source": [
        "def Ncontrast(x_dis, adj_label, tau = 1):\n",
        "    \"\"\"\n",
        "    compute the Ncontrast loss\n",
        "    \"\"\"\n",
        "    x_dis = torch.exp( tau * x_dis)\n",
        "    x_dis_sum = torch.sum(x_dis, 1)\n",
        "    x_dis_sum_pos = torch.sum(x_dis*adj_label, 1)\n",
        "    loss = -torch.log(x_dis_sum_pos * (x_dis_sum**(-1))+1e-8).mean()\n",
        "    return loss\n",
        "\n",
        "def get_batch(batch_size, idx_train, adj_label, features):\n",
        "    \"\"\"\n",
        "    get a batch of feature & adjacency matrix\n",
        "    \"\"\"\n",
        "    rand_indx = torch.tensor(np.random.choice(np.arange(adj_label.shape[0]), batch_size)).type(torch.long).cuda()\n",
        "    rand_indx[0:len(idx_train)] = idx_train\n",
        "    features_batch = features[rand_indx]\n",
        "    adj_label_batch = adj_label[rand_indx,:][:,rand_indx]\n",
        "    return features_batch, adj_label_batch\n",
        "\n",
        "def train(model, CL, optimizer, s_idx0, s_idx1, bs, KL_div, tau, alpha, beta, idx_train, adj_label, features, Y, MSEL):\n",
        "    features_batch, adj_label_batch = get_batch(bs, idx_train, adj_label, features)\n",
        "    model.train()\n",
        "    CL.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output, x_dis, embeddings = model(features_batch)\n",
        "\n",
        "    output = CL(embeddings)\n",
        "    output0, output1 = output[s_idx0], output[s_idx1]\n",
        "    target0, target1 = target_distribution(output0).detach(), target_distribution(output1).detach()\n",
        "    fair_loss = 0.5 * KL_div(output0.log(), target0) + 0.5 * KL_div(output1.log(), target1)\n",
        "\n",
        "    loss_Ncontrast = Ncontrast(x_dis, adj_label_batch, tau = tau)\n",
        "\n",
        "    predict0, predict1 = Y[s_idx0], Y[s_idx1]\n",
        "    partition_loss = 0.5 * MSEL(aff(output0), aff(predict0)) + 0.5 * MSEL(aff(output1), aff(predict1))\n",
        "\n",
        "    loss_train = alpha * fair_loss + loss_Ncontrast + beta * partition_loss\n",
        "\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aLy3PyilApIW"
      },
      "outputs": [],
      "source": [
        "def ConsensusFairClusteringHelper(name, X_in, s_in, y_in, save, order=1, lr=0.01, weight_decay=5e-3, alpha=50.0, num_hidden=256, bs=3800, tau=2, epochs=3000, dropout=0.6):\n",
        "  k = len(np.unique(y_in))\n",
        "\n",
        "  if name == 'Office-31':\n",
        "    beta = 100.0\n",
        "    alpha = 1.0\n",
        "    order = 1\n",
        "  if name == 'MNIST_USPS':\n",
        "    beta = 25.0\n",
        "    alpha = 100.0\n",
        "    order = 2\n",
        "  if name == 'Yale':\n",
        "    beta = 10.0\n",
        "    alpha = 50.0\n",
        "    order = 2\n",
        "  if name == 'DIGITS':\n",
        "    beta = 50.0\n",
        "    alpha = 10.0\n",
        "    order = 2\n",
        "    num_hidden=36\n",
        "\n",
        "\n",
        "  ckm = CKmeans(k=k, n_rep=100, p_samp=0.5, p_feat=0.5, random_state=42)\n",
        "  ckm.fit(X_in)\n",
        "  ckm_res = ckm.predict(X_in, return_cls=True)\n",
        "\n",
        "\n",
        "  adj, features, labels = ckm_res.cmatrix, X_in, y_in\n",
        "  adj = sparse.csr_matrix(adj)\n",
        "  adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
        "  features = torch.FloatTensor(features).float()\n",
        "  labels = torch.LongTensor(labels)\n",
        "  idx_train = np.array(range(len(features)))\n",
        "  idx_train = torch.LongTensor(idx_train)\n",
        "\n",
        "  adj_label = get_A_r(adj, order)\n",
        "  adj, adj_label, features, idx_train = adj.cuda(), adj_label.cuda(), features.cuda(), idx_train.cuda()\n",
        "\n",
        "  s_idx0, s_idx1 = [], []\n",
        "  for i in range(len(s_in)):\n",
        "    if s_in[i] == 0:\n",
        "      s_idx0.append(i)\n",
        "    elif s_in[i] == 1:\n",
        "      s_idx1.append(i)\n",
        "\n",
        "\n",
        "  L = np.load('Consensus-Fair-Clustering/precomputed_labels/labels_' + name + '.npy')\n",
        "  Y = np.zeros((len(s), k))\n",
        "  for i,l in enumerate(L):\n",
        "    Y[i,l] = 1.0\n",
        "  Y = torch.FloatTensor(Y).float().cuda()\n",
        "  MSEL = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "  torch.use_deterministic_algorithms(True)\n",
        "  model = GMLP(nfeat=features.shape[1],\n",
        "              nhid=num_hidden,\n",
        "              nclass=labels.max().item() + 1,\n",
        "              dropout=dropout,\n",
        "              )\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "  torch.use_deterministic_algorithms(True)\n",
        "  CL = ClusteringLayer(cluster_number=k, hidden_dimension=num_hidden).cuda()\n",
        "\n",
        "  optimizer = optim.Adam(model.get_parameters() + CL.get_parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  KL_div = nn.KLDivLoss(reduction=\"sum\")\n",
        "  model.cuda()\n",
        "  features = features.cuda()\n",
        "  labels = labels.cuda()\n",
        "  idx_train = idx_train.cuda()\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train(model, CL, optimizer, s_idx0, s_idx1, bs, KL_div, tau, alpha, beta, idx_train, adj_label, features, Y, MSEL)\n",
        "\n",
        "  model.eval()\n",
        "  logits, embeddings = model(features)\n",
        "  CL.eval()\n",
        "  preds = CL(embeddings)\n",
        "  preds = preds.cpu().detach().numpy()\n",
        "  pred_labels = np.argmax(preds, axis=1)\n",
        "\n",
        "  return pred_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LWYGV4EkW8Zt"
      },
      "outputs": [],
      "source": [
        "def ConsensusFairClustering(name, X_in, s_in, y_in, save):\n",
        "  name_bal = {'Office-31': 0.5, 'MNIST_USPS': 0.3, 'DIGITS': 0.1, 'Yale': 0.1}\n",
        "  while True: #Sometimes the model optimizes for a local minima which is why we can run enough times to get a good representation learnt\n",
        "    cfc_labels = ConsensusFairClusteringHelper(name, X_in, s_in, y_in, save)\n",
        "    if balance(cfc_labels, X_in, s_in) >= name_bal[name]: #threshold -> 0.5 for Office-31 and 0.3 (0.4) for MNIST_USPS and 0.1 for DIGITS and 0.1 for Yale\n",
        "      break\n",
        "  print(\"\\nCompleted CFC model training.\")\n",
        "  return cfc_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ2BngWantBP",
        "outputId": "1df65a26-0f90-4ca6-c63d-337f5bfcddb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [03:49<00:00, 13.05it/s]\n",
            "100%|██████████| 3000/3000 [03:47<00:00, 13.17it/s]\n",
            "100%|██████████| 3000/3000 [03:51<00:00, 12.96it/s]\n",
            "100%|██████████| 3000/3000 [03:49<00:00, 13.07it/s]\n",
            "100%|██████████| 3000/3000 [03:47<00:00, 13.16it/s]\n",
            "100%|██████████| 3000/3000 [03:47<00:00, 13.20it/s]\n",
            "100%|██████████| 3000/3000 [03:46<00:00, 13.23it/s]\n",
            "100%|██████████| 3000/3000 [03:51<00:00, 12.96it/s]\n",
            "100%|██████████| 3000/3000 [03:49<00:00, 13.06it/s]\n",
            "100%|██████████| 3000/3000 [03:49<00:00, 13.05it/s]\n",
            "100%|██████████| 3000/3000 [03:48<00:00, 13.10it/s]\n",
            "100%|██████████| 3000/3000 [03:52<00:00, 12.89it/s]\n",
            "100%|██████████| 3000/3000 [03:53<00:00, 12.87it/s]\n",
            "100%|██████████| 3000/3000 [03:50<00:00, 13.02it/s]\n",
            "100%|██████████| 3000/3000 [03:50<00:00, 13.00it/s]\n",
            "100%|██████████| 3000/3000 [02:19<00:00, 21.50it/s]\n",
            "100%|██████████| 3000/3000 [03:51<00:00, 12.97it/s]\n",
            "100%|██████████| 3000/3000 [03:55<00:00, 12.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Completed CFC model training.\n",
            "[9 2 2 ... 2 2 9]\n"
          ]
        }
      ],
      "source": [
        "# Trial run!\n",
        "lbls = ConsensusFairClustering(name, X, s, y, save=False)\n",
        "print(lbls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykQKdAB2n8Hn",
        "outputId": "8003433a-531b-4321-dc34-c7916d039db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "balance: 0.3986013986013986\n",
            "entropy: 2.8046766891714006\n",
            "nmi: 0.2629514544169175\n",
            "acc: 0.3821052631578947\n"
          ]
        }
      ],
      "source": [
        "# Check to see metrics too!\n",
        "print(\"balance: {}\".format(balance(lbls, X, s)))\n",
        "print(\"entropy: {}\".format(entropy(lbls, s)))\n",
        "print(\"nmi: {}\".format(nmi(y, lbls)))\n",
        "print(\"acc: {}\".format(acc(y, lbls)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FZ0t_hnMNWDA"
      },
      "outputs": [],
      "source": [
        "''' the function that the attacker wants to optimize, computes the balance of the modified protected groups clustering'''\n",
        "def attack_balance(solution):\n",
        "  X_copy, s_copy = X.copy(), s.copy()\n",
        "  flipped_labels = solution.get_x() #retrieve the solution from the optimization process and assigning it to the variable flipped_labels\n",
        "  i = 0\n",
        "  # for index in U_idx (all the indixes the attacker has access to)\n",
        "  for idx in U_idx:\n",
        "    # change the correct protected group of that sample with the one optimal for the attacker\n",
        "    s_copy[idx] = flipped_labels[i]\n",
        "    i += 1\n",
        "  # compute the clustering again, but with the modified protected groups\n",
        "  labels_sfd = ConsensusFairClustering(name, X_copy, s_copy, y, save=False)\n",
        "\n",
        "  s_eval = []\n",
        "  X_eval = []\n",
        "  labels_sfd_eval = []\n",
        "  # evaluate the solution given by the algo over the whole dataset, iterating through all the indices\n",
        "  for idx in V_idx:\n",
        "    s_eval.append(s_copy[idx])\n",
        "    X_eval.append(X_copy[idx])\n",
        "    labels_sfd_eval.append(labels_sfd[idx])\n",
        "  s_eval = np.array(s_eval)\n",
        "  X_eval = np.array(X_eval)\n",
        "  labels_sfd_eval = np.array(labels_sfd_eval)\n",
        "  #calculate the balance\n",
        "  bal = balance(labels_sfd_eval, X_eval, s_eval)\n",
        "\n",
        "  return bal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-D3QlGghN_2o"
      },
      "outputs": [],
      "source": [
        "''' attacks the clustering algo and computes the metrics of to the attacked clustering '''\n",
        "\n",
        "def process_solution(sol):\n",
        "  X_copy, s_copy, y_copy = X.copy(), s.copy(), y.copy() # X is the data, y is the ground truth value and s is the protected group\n",
        "  flipped_labels = sol.get_x() #retrieve the solution from the optimization process and assigning it to the variable flipped_labels\n",
        "  i = 0\n",
        "  # for index in U_idx (all the indixes the attacker has access to)\n",
        "  for idx in U_idx:\n",
        "    # change the correct protected group of that sample with the one optimal for the attacker\n",
        "    s_copy[idx] = flipped_labels[i]\n",
        "    i += 1\n",
        "\n",
        "  # compute the clustering again, but with the modified protected groups\n",
        "  labels_sfd = ConsensusFairClustering(name, X_copy, s_copy, y, save=False)\n",
        "\n",
        "  s_eval = []\n",
        "  X_eval = []\n",
        "  labels_sfd_eval = []\n",
        "  y_eval = []\n",
        "\n",
        "  # evaluate the solution given by the algo over the whole dataset, iterating through all the indices\n",
        "  for idx in V_idx:\n",
        "    s_eval.append(s_copy[idx]) # append the modified protected groups\n",
        "    X_eval.append(X_copy[idx]) # append the samples\n",
        "    labels_sfd_eval.append(labels_sfd[idx]) # append the clusters\n",
        "    y_eval.append(y_copy[idx]) # append the ground truth values\n",
        "\n",
        "  # convert to numpy arrays\n",
        "  s_eval = np.array(s_eval)\n",
        "  X_eval = np.array(X_eval)\n",
        "  labels_sfd_eval = np.array(labels_sfd_eval)\n",
        "  y_eval = np.array(y_eval)\n",
        "\n",
        "  # calculate the metrics and return them\n",
        "  bal = balance(labels_sfd_eval, X_eval, s_eval)\n",
        "  ent = entropy(labels_sfd_eval, s_eval)\n",
        "  accuracy = acc(y_eval, labels_sfd_eval)\n",
        "  nmi_score = nmi(y_eval, labels_sfd_eval)\n",
        "\n",
        "  return (bal, ent, accuracy, nmi_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htHIs8odOGPE",
        "outputId": "eeaa4b8d-ed50-45bf-8c04-4c747495320b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of clusters -> 10\n",
            "percent 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [03:53<00:00, 12.84it/s]\n",
            "100%|██████████| 3000/3000 [03:51<00:00, 12.93it/s]\n",
            "100%|██████████| 3000/3000 [03:49<00:00, 13.05it/s]\n",
            "100%|██████████| 3000/3000 [03:56<00:00, 12.68it/s]\n",
            "100%|██████████| 3000/3000 [03:51<00:00, 12.94it/s]\n",
            "100%|██████████| 3000/3000 [03:51<00:00, 12.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Completed CFC model training.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [02:18<00:00, 21.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Completed CFC model training.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [03:52<00:00, 12.92it/s]\n",
            "100%|██████████| 3000/3000 [03:51<00:00, 12.93it/s]\n",
            " 82%|████████▏ | 2452/3000 [03:08<00:47, 11.58it/s]"
          ]
        }
      ],
      "source": [
        "# number of cluster is the number of all possible ground-truth values\n",
        "n_clusters = len(np.unique(y))\n",
        "print(\"# of clusters -> \" + str(n_clusters))\n",
        "n_trials = 1\n",
        "\n",
        "U_idx_full, V_idx_full = np.load('U_idx_' + name + '.npy').tolist(), np.load('V_idx_' + name + '.npy').tolist()\n",
        "\n",
        "cfc_pre_res = {\n",
        "    0 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    1 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    2 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    3 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    4 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    5 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    6 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    7 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "}\n",
        "\n",
        "\n",
        "cfc_post_res = {\n",
        "    0 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    1 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    2 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    3 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    4 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    5 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    6 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "    7 : {'BALANCE': [], 'ENTROPY': [], 'ACC': [], 'NMI': []},\n",
        "}\n",
        "\n",
        "#U_idx is the list with all the indices of the samples, which might have the group membership switched\n",
        "for percent, j in enumerate([int(0.125*len(U_idx_full)), int(0.25*len(U_idx_full)), int(0.375*len(U_idx_full)), int(0.5*len(U_idx_full)), int(0.625*len(U_idx_full)), int(0.75*len(U_idx_full)), int(0.875*len(U_idx_full)), int(len(U_idx_full))]):\n",
        "  print('percent', percent)\n",
        "  U_idx = U_idx_full[:j] #take the portion of U_idx\n",
        "  V_idx = V_idx_full #indices of the full dataset\n",
        "\n",
        "  for trial_idx in range(n_trials):\n",
        "    #compute the clusters using CFC\n",
        "    labels = ConsensusFairClustering(name, X, s, y, save=False) # X is the data, y is the ground truth value and s is the protected group\n",
        "    \n",
        "    s_test = []\n",
        "    X_test = []\n",
        "    labels_test = []\n",
        "    y_test = []\n",
        "    # for each index in all the indices of the dataset\n",
        "    for idx in V_idx:\n",
        "      s_test.append(s[idx]) # append the protected group of that sample \n",
        "      X_test.append(X[idx]) # append the data of that sample \n",
        "      labels_test.append(labels[idx]) # append the cluster assigned to that sample\n",
        "      y_test.append(y[idx]) # append the real value of that sample\n",
        "\n",
        "    # convert to numpy array\n",
        "    s_test = np.array(s_test)\n",
        "    X_test = np.array(X_test)\n",
        "    labels_test = np.array(labels_test)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    # add to the pre-attack dictionary the calculated metrics (balance, entropy, accuracy and nmi)\n",
        "    cfc_pre_res[percent]['BALANCE'].append(balance(labels_test, X_test, s_test))\n",
        "    cfc_pre_res[percent]['ENTROPY'].append(entropy(labels_test, s_test))\n",
        "    cfc_pre_res[percent]['ACC'].append(acc(y_test, labels_test))\n",
        "    cfc_pre_res[percent]['NMI'].append(nmi(y_test, labels_test))\n",
        "\n",
        "    # dim_size is the size of the indexes the attacker can modify at this point\n",
        "    dim_size = len(U_idx)\n",
        "    # Dimension is used to define the search space for optimization algorithms. dim_size indicated how many variables we want to optimize\n",
        "    # the second parameter defines the range of values. Last parameter specifies that all variables have to be continuous.\n",
        "    # So we are defining a search space for an optimization problem with dim_size dimensions, where each dimension allows continuous variables in the range [0, 1].\n",
        "    dim = Dimension(dim_size, [[0, 1]]*dim_size, [False]*dim_size)\n",
        "    #here we are optimizing the attack, with respect to the function attack_balance which is defined above and 'dim', which defines the search space for the optimization problem\n",
        "    obj = Objective(attack_balance, dim)\n",
        "    # we perform minimization optimization on obj (the optimization problem). The second parameter represents the maximum number of function \n",
        "    # evaluations or iterations that the optimization algorithm is allowed to perform ( 5 in this case )\n",
        "    # the output stores the result of finding the values of the variables within the search space (dim) that minimize the \n",
        "    # objective function (attack_balance) and doing so within a maximum budget of 5 evaluations.\n",
        "    solution = Opt.min(obj, Parameter(budget=5))\n",
        "    \n",
        "    # attack the algorithm and calculate the metrics with respect to the clustering post-attack\n",
        "    pa_bal, pa_ent, pa_acc, pa_nmi = process_solution(solution)\n",
        "\n",
        "    # store the post-attack metrics in the dictionary\n",
        "    cfc_post_res[percent]['BALANCE'].append(pa_bal)\n",
        "    cfc_post_res[percent]['ENTROPY'].append(pa_ent)\n",
        "    cfc_post_res[percent]['ACC'].append(pa_acc)\n",
        "    cfc_post_res[percent]['NMI'].append(pa_nmi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYZNLnq6W1s8"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml1labs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5 | packaged by conda-forge | (default, Jun 19 2021, 00:27:35) \n[Clang 11.1.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "62cae76669e99b94d658b8159e4ccc316e6545dadb04e2b555d505da057778cf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
